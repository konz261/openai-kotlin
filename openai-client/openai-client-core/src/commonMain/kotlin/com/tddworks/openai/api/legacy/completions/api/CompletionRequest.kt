package com.tddworks.openai.api.legacy.completions.api

import com.tddworks.common.network.api.ktor.api.AnySerial
import com.tddworks.openai.api.chat.api.OpenAIModel
import kotlinx.serialization.EncodeDefault
import kotlinx.serialization.ExperimentalSerializationApi
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

/** https://platform.openai.com/docs/api-reference/completions/create */
@Serializable
@ExperimentalSerializationApi
data class CompletionRequest(

    /**
     * Controls whether the target property is serialized when its value is equal to a default
     * value, regardless of the format settings. Does not affect decoding and deserialization
     * process.
     *
     * Example of usage:
     * ```
     * @Serializable
     * data class Foo(
     *     @EncodeDefault(ALWAYS) val a: Int = 42,
     *     @EncodeDefault(NEVER) val b: Int = 43,
     *     val c: Int = 44
     * )
     *
     * Json { encodeDefaults = false }.encodeToString((Foo()) // {"a": 42}
     * Json { encodeDefaults = true }.encodeToString((Foo())  // {"a": 42, "c":44}
     * ```
     *
     * @see EncodeDefault.Mode.ALWAYS
     * @see EncodeDefault.Mode.NEVER
     */
    @EncodeDefault(EncodeDefault.Mode.ALWAYS)
    @SerialName("model")
    val model: OpenAIModel = OpenAIModel.GPT_3_5_TURBO_INSTRUCT,

    /**
     * prompt string or array
     *
     * Required The prompt(s) to generate completions for, encoded as a string, array of strings,
     * array of tokens, or array of token arrays.
     *
     * Note that <|endoftext|> is the document separator that the model sees during training, so if
     * a prompt is not specified the model will generate as if from the beginning of a new document.
     */
    @SerialName("prompt") val prompt: String,

    /**
     * best_of integer or null
     *
     * Optional Defaults to 1 Generates best_of completions server-side and returns the "best" (the
     * one with the highest log probability per token). Results cannot be streamed.
     *
     * When used with n, best_of controls the number of candidate completions and n specifies how
     * many to return â€“ best_of must be greater than n.
     *
     * Note: Because this parameter generates many completions, it can quickly consume your token
     * quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop
     */
    @SerialName("best_of") val bestOf: Int? = null,

    /**
     * echo boolean or null
     *
     * Optional Defaults to false Echo back the prompt in addition to the completion
     */
    @SerialName("echo") val echo: Boolean? = null,

    /**
     * number or null Optional Defaults to 0 Number between -2.0 and 2.0. Positive values penalize
     * new tokens based on their existing frequency in the text so far, decreasing the model's
     * likelihood to repeat the same line verbatim
     */
    @SerialName("frequency_penalty") val frequencyPenalty: Double? = null,

    /**
     * map Optional Defaults to null Modify the likelihood of specified tokens appearing in the
     * completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an
     * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
     * generated by the model prior to sampling. The exact effect will vary per model, but values
     * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    @SerialName("logit_bias") val logitBias: Map<String, Int>? = null,

    /**
     * boolean or null Optional Defaults to false Whether to return log probabilities of the output
     * tokens or not. If true, returns the log probabilities of each output token returned in the
     * content of message. This option is currently not available on the gpt-4-vision-preview model.
     */
    @SerialName("logprobs") val logprobs: Boolean? = null,

    /**
     * integer or null Optional The maximum number of tokens that can be generated in the chat
     * completion.
     *
     * The total length of input tokens and generated tokens is limited by the model's context
     * length.
     */
    @SerialName("max_tokens") val maxTokens: Int? = null,

    /**
     * integer or null Optional Defaults to 1 How many chat completion choices to generate for each
     * input message. Note that you will be charged based on the number of generated tokens across
     * all of the choices. Keep n as 1 to minimize costs.
     */
    @SerialName("n") val n: Int? = null,

    /**
     * number or null Optional Defaults to 0 Number between -2.0 and 2.0. Positive values penalize
     * new tokens based on whether they appear in the text so far, increasing the model's likelihood
     * to talk about new topics.
     */
    @SerialName("presence_penalty") val presencePenalty: Double? = null,

    /**
     * integer or null Optional This feature is in Beta. If specified, our system will make a best
     * effort to sample deterministically, such that repeated requests with the same seed and
     * parameters should return the same result. Determinism is not guaranteed, and you should refer
     * to the system_fingerprint response parameter to monitor changes in the backen
     */
    @SerialName("seed") val seed: Int? = null,

    /**
     * string / array / null Optional Defaults to null Up to 4 sequences where the API will stop
     * generating further tokens.
     */
    @SerialName("stop") val stop: String? = null,

    /**
     * stream boolean or null
     *
     * Optional Defaults to false Whether to stream back partial progress. If set, tokens will be
     * sent as data-only server-sent events as they become available, with the stream terminated by
     * a data: [DONE] message. Example Python code.
     */
    @SerialName("stream") val stream: Boolean? = null,

    /**
     * include_usage boolean
     *
     * Optional If set, an additional chunk will be streamed before the data: [DONE] message. The
     * usage field on this chunk shows the token usage statistics for the entire request, and the
     * choices field will always be an empty array. All other chunks will also include a usage
     * field, but with a null value.
     */
    @SerialName("stream_options") val streamOptions: Map<String, AnySerial>? = null,

    /**
     * suffix string or null
     *
     * Optional Defaults to null The suffix that comes after a completion of inserted text.
     *
     * This parameter is only supported for gpt-3.5-turbo-instruct.
     */
    val suffix: String? = null,

    /**
     * number or null Optional Defaults to 1 What sampling temperature to use, between 0 and 2.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will
     * make it more focused and deterministic.
     *
     * We generally recommend altering this or top_p but not both
     */
    @SerialName("temperature") val temperature: Double? = null,

    /**
     * number or null Optional Defaults to 1 An alternative to sampling with temperature, called
     * nucleus sampling, where the model considers the results of the tokens with top_p probability
     * mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    @SerialName("top_p") val topP: Double? = null,

    /**
     * string Optional A unique identifier representing your end-user, which can help OpenAI to
     * monitor and detect abuse.
     */
    @SerialName("user") val user: String? = null,
) {
    companion object {
        fun asStream(
            model: OpenAIModel = OpenAIModel.GPT_3_5_TURBO_INSTRUCT,
            prompt: String,
            bestOf: Int? = null,
            echo: Boolean? = null,
            frequencyPenalty: Double? = null,
            logitBias: Map<String, Int>? = null,
            logprobs: Boolean? = null,
            maxTokens: Int? = null,
            n: Int? = null,
            presencePenalty: Double? = null,
            seed: Int? = null,
            stop: String? = null,
            streamOptions: Map<String, AnySerial>? = null,
            suffix: String? = null,
            temperature: Double? = null,
            topP: Double? = null,
            user: String? = null,
        ) =
            CompletionRequest(
                model = model,
                prompt = prompt,
                bestOf = bestOf,
                echo = echo,
                frequencyPenalty = frequencyPenalty,
                logitBias = logitBias,
                logprobs = logprobs,
                maxTokens = maxTokens,
                n = n,
                presencePenalty = presencePenalty,
                seed = seed,
                stop = stop,
                stream = true,
                streamOptions = streamOptions,
                suffix = suffix,
                temperature = temperature,
                topP = topP,
                user = user,
            )
    }
}
