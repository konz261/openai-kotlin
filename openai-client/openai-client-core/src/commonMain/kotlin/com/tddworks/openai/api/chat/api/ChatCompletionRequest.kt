package com.tddworks.openai.api.chat.api

import kotlinx.serialization.*

/**
 * Data class representing the settings for generating a chat completion.
 *
 * @property messages The user's message.
 * @property openAIModel The GPT model to use for generating the chat completion. Default is
 *   GPT_3_5_TURBO.
 * @see [Chat documentation](https://beta.openai.com/docs/api-reference/chat)
 */
@Serializable
@ExperimentalSerializationApi
data class ChatCompletionRequest(
    /**
     * A list of messages comprising the conversation so far. possible types System message User
     * message Assistant message Tool message Function message
     */
    @SerialName("messages") val messages: List<ChatMessage>,

    /**
     * Controls whether the target property is serialized when its value is equal to a default
     * value, regardless of the format settings. Does not affect decoding and deserialization
     * process.
     *
     * Example of usage:
     * ```
     * @Serializable
     * data class Foo(
     *     @EncodeDefault(ALWAYS) val a: Int = 42,
     *     @EncodeDefault(NEVER) val b: Int = 43,
     *     val c: Int = 44
     * )
     *
     * Json { encodeDefaults = false }.encodeToString((Foo()) // {"a": 42}
     * Json { encodeDefaults = true }.encodeToString((Foo())  // {"a": 42, "c":44}
     * ```
     *
     * @see EncodeDefault.Mode.ALWAYS
     * @see EncodeDefault.Mode.NEVER
     */
    @EncodeDefault(EncodeDefault.Mode.ALWAYS)
    @SerialName("model")
    val model: OpenAIModel = OpenAIModel.GPT_3_5_TURBO,

    /**
     * number or null Optional Defaults to 0 Number between -2.0 and 2.0. Positive values penalize
     * new tokens based on their existing frequency in the text so far, decreasing the model's
     * likelihood to repeat the same line verbatim
     */
    @SerialName("frequency_penalty") val frequencyPenalty: Double? = null,

    /**
     * map Optional Defaults to null Modify the likelihood of specified tokens appearing in the
     * completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an
     * associated bias value from -100 to 100. Mathematically, the bias is added to the logits
     * generated by the model prior to sampling. The exact effect will vary per model, but values
     * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     */
    @SerialName("logit_bias") val logitBias: Map<String, Int>? = null,

    /**
     * boolean or null Optional Defaults to false Whether to return log probabilities of the output
     * tokens or not. If true, returns the log probabilities of each output token returned in the
     * content of message. This option is currently not available on the gpt-4-vision-preview model.
     */
    @SerialName("logprobs") val logprobs: Boolean? = null,

    /**
     * integer or null Optional An integer between 0 and 5 specifying the number of most likely
     * tokens to return at each token position, each with an associated log probability. logprobs
     * must be set to true if this parameter is used.
     */
    @SerialName("top_logprobs") val topLogprobs: Int? = null,

    /**
     * integer or null Optional The maximum number of tokens that can be generated in the chat
     * completion.
     *
     * The total length of input tokens and generated tokens is limited by the model's context
     * length.
     */
    @SerialName("max_tokens") val maxTokens: Int? = null,

    /**
     * integer or null Optional Defaults to 1 How many chat completion choices to generate for each
     * input message. Note that you will be charged based on the number of generated tokens across
     * all of the choices. Keep n as 1 to minimize costs.
     */
    @SerialName("n") val n: Int? = null,

    /**
     * number or null Optional Defaults to 0 Number between -2.0 and 2.0. Positive values penalize
     * new tokens based on whether they appear in the text so far, increasing the model's likelihood
     * to talk about new topics.
     */
    @SerialName("presence_penalty") val presencePenalty: Double? = null,

    /**
     * object Optional An object specifying the format that the model must output. Compatible with
     * GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.
     *
     * Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the
     * model generates is valid JSON.
     *
     * Important: when using JSON mode, you must also instruct the model to produce JSON yourself
     * via a system or user message. Without this, the model may generate an unending stream of
     * whitespace until the generation reaches the token limit, resulting in a long-running and
     * seemingly "stuck" request. Also note that the message content may be partially cut off if
     * finish_reason="length", which indicates the generation exceeded max_tokens or the
     * conversation exceeded the max context length.
     *
     * type string Optional Defaults to text Must be one of text or json_object.
     */
    @SerialName("response_format") val responseFormat: String? = null,

    /**
     * integer or null Optional This feature is in Beta. If specified, our system will make a best
     * effort to sample deterministically, such that repeated requests with the same seed and
     * parameters should return the same result. Determinism is not guaranteed, and you should refer
     * to the system_fingerprint response parameter to monitor changes in the backen
     */
    @SerialName("seed") val seed: Int? = null,

    /**
     * string / array / null Optional Defaults to null Up to 4 sequences where the API will stop
     * generating further tokens.
     */
    @SerialName("stop") val stop: String? = null,

    /**
     * number or null Optional Defaults to 1 What sampling temperature to use, between 0 and 2.
     * Higher values like 0.8 will make the output more random, while lower values like 0.2 will
     * make it more focused and deterministic.
     *
     * We generally recommend altering this or top_p but not both
     */
    @SerialName("temperature") val temperature: Double? = null,

    /**
     * number or null Optional Defaults to 1 An alternative to sampling with temperature, called
     * nucleus sampling, where the model considers the results of the tokens with top_p probability
     * mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     */
    @SerialName("top_p") val topP: Double? = null,
    val v: String? = null,
    val stream: Boolean? = null,

    /**
     * array Optional A list of tools the model may call. Currently, only functions are supported as
     * a tool. Use this to provide a list of functions the model may generate JSON inputs for.
     *
     * properties type string Required The type of the tool. Currently, only function is supported.
     *
     * function object Required
     *
     * Hide properties description string Optional A description of what the function does, used by
     * the model to choose when and how to call the function.
     *
     * name string Required The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
     * underscores and dashes, with a maximum length of 64.
     *
     * parameters object Optional The parameters the functions accepts, described as a JSON Schema
     * object. See the guide for examples, and the JSON Schema reference for documentation about the
     * format.
     *
     * Omitting parameters defines a function with an empty parameter list.
     */
    @SerialName("tools") val tools: List<String>? = null,

    /**
     * string or object Optional Controls which (if any) function is called by the model. none means
     * the model will not call a function and instead generates a message. auto means the model can
     * pick between generating a message or calling a function. Specifying a particular function via
     * {"type": "function", "function": {"name": "my_function"}} forces the model to call that
     * function.
     *
     * none is the default when no functions are present. auto is the default if functions are
     * present.
     *
     * possible types
     *
     * string none means the model will not call a function and instead generates a message. auto
     * means the model can pick between generating a message or calling a function.
     *
     * object Specifies a tool the model should use. Use to force the model to call a specific
     * function. properties type string Required The type of the tool. Currently, only function is
     * supported. function object Required properties name string Required The name of the function
     * to call.
     */
    @SerialName("tool_choice") val toolChoice: String? = null,

    /**
     * string Optional A unique identifier representing your end-user, which can help OpenAI to
     * monitor and detect abuse.
     */
    @SerialName("user") val user: String? = null,
) {

    companion object {
        fun chatCompletionRequest(
            messages: List<ChatMessage>,
            openAIModel: OpenAIModel = OpenAIModel.GPT_3_5_TURBO,
        ): ChatCompletionRequest {
            return ChatCompletionRequest(messages = messages, model = openAIModel)
        }

        fun chatCompletionsRequest(
            messages: List<ChatMessage>,
            model: OpenAIModel = OpenAIModel.GPT_3_5_TURBO,
        ): ChatCompletionRequest {
            return ChatCompletionRequest(messages = messages, model = model)
        }

        /**
         * Convenience function to create a ComplicationRequest from a string question.
         *
         * @param question The user's question.
         * @return A ComplicationRequest object.
         */
        fun from(question: String): ChatCompletionRequest {
            return ChatCompletionRequest(listOf(ChatMessage.UserMessage(question)))
        }

        fun from(question: String, model: OpenAIModel): ChatCompletionRequest {
            return ChatCompletionRequest(
                messages = listOf(ChatMessage.UserMessage(question)),
                model = model,
            )
        }

        /**
         * Creates a dummy ComplicationRequest for testing purposes.
         *
         * @return A ComplicationRequest object with a single dummy message.
         */
        fun dummy(model: OpenAIModel): ChatCompletionRequest {
            return ChatCompletionRequest(
                listOf(ChatMessage.UserMessage("Hello! How can I assist you today?")),
                model = model,
            )
        }
    }
}
